\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\graphicspath{{../images/}}
\usepackage{hyperref}
\geometry{margin=1in}

\title{Statistics and Data Analysis\\Unit 04 -- Lecture 08 Notes\\Cross-validation and Hyper-parameter Tuning}
\author{Tofik Ali}
\date{\today}

\begin{document}
\maketitle

\section*{Topic}
Train/test split, k-fold cross-validation, and hyper-parameter tuning (grid/random search).
\section*{How to Use These Notes}
These notes are written for students who are seeing the topic for the first time. They
follow the slide order, but add the missing 'why', interpretation, and common mistakes. If
you get stuck, look at the worked exercises and then run the Python demo.

Course repository (slides, demos, datasets): \url{https://github.com/tali7c/Statistics-and-Data-Analysis}

\section*{Time Plan (55 minutes)}
\begin{itemize}
  \item 0--10 min: Attendance + recap of previous lecture
  \item 10--35 min: Core concepts (this lecture's sections)
  \item 35--45 min: Exercises (solve 1--2 in class, rest as practice)
  \item 45--50 min: Mini demo + interpretation of output
  \item 50--55 min: Buffer / wrap-up (leave 5 minutes early)
\end{itemize}

\section*{Slide-by-slide Notes}
\subsection*{Title Slide}
State the lecture title clearly and connect it to what students already know.
Tell students what they will be able to do by the end (not just what you will cover).

\subsection*{Quick Links / Agenda}
Explain the structure of the lecture and where the exercises and demo appear.
\begin{itemize}
  \item Overview
  \item Cross-validation
  \item Hyper-parameter Tuning
  \item Exercises
  \item Demo
  \item Summary
\end{itemize}

\subsection*{Learning Outcomes}
\begin{itemize}
  \item Explain train/validation/test split roles
  \item Describe k-fold cross-validation
  \item Explain grid search vs random search
  \item Avoid data leakage using pipelines
\end{itemize}
\paragraph{Why these outcomes matter.}
\textbf{Cross-validation (CV)} repeatedly splits data into train/validation parts to
estimate generalization more reliably. It reduces the dependence on a single lucky/unlucky
split and is essential for selecting hyperparameters without touching the test set.
\textbf{Grid search} tries every combination of hyperparameter values in a predefined grid.
It is simple but can be expensive. \textbf{Random search} can be more efficient when only a
few hyperparameters really matter.

\subsection*{Cross-validation: Key Points}
\begin{itemize}
  \item CV estimates generalization more stably than one split
  \item k-fold repeats train/validate across folds
  \item Average score guides selection
\end{itemize}
\paragraph{Explanation.}
\textbf{Cross-validation (CV)} repeatedly splits data into train/validation parts to
estimate generalization more reliably. It reduces the dependence on a single lucky/unlucky
split and is essential for selecting hyperparameters without touching the test set.

\subsection*{Hyper-parameter Tuning: Key Points}
\begin{itemize}
  \item Grid search tries all combos
  \item Random search samples combos efficiently
  \item Never tune on the test set
\end{itemize}
\paragraph{Explanation.}
A \textbf{parameter} is a fixed (but usually unknown) number that describes the population
(e.g., $\mu$, $\sigma$). A \textbf{statistic} is a number computed from the sample (e.g.,
$\bar{x}$, $s$). Statistics vary from sample to sample, which is why we talk about
uncertainty.
\textbf{Grid search} tries every combination of hyperparameter values in a predefined grid.
It is simple but can be expensive. \textbf{Random search} can be more efficient when only a
few hyperparameters really matter.

\subsection*{Exercises (with Solutions)}
Attempt the exercise first, then compare with the solution. Focus on interpretation, not
only arithmetic.

\subsection*{Exercise 1: Grid size}
3 parameters with 4 values each: how many combinations?
\subsubsection*{Solution}
\begin{itemize}
  \item $4^3 = 64$
\end{itemize}
\paragraph{Walkthrough.}
A \textbf{parameter} is a fixed (but usually unknown) number that describes the population
(e.g., $\mu$, $\sigma$). A \textbf{statistic} is a number computed from the sample (e.g.,
$\bar{x}$, $s$). Statistics vary from sample to sample, which is why we talk about
uncertainty.

\subsection*{Exercise 2: Leakage}
Is scaling on full dataset before split leakage?
\subsubsection*{Solution}
\begin{itemize}
  \item Yes; fit preprocessing on training only.
\end{itemize}
\paragraph{Walkthrough.}
An \textbf{ROC curve} plots true positive rate vs false positive rate across all thresholds.
It helps compare classifiers without committing to a single threshold. \textbf{AUC}
summarizes the ROC curve: higher AUC generally indicates better ranking of positives above
negatives.
\textbf{Data leakage} happens when information from the future or from the test set
influences training. Typical examples: scaling before splitting, using target-related
features, or using random splits for time series. Leakage can produce very good-looking
accuracy that disappears in real deployment.

\subsection*{Exercise 3: Why CV}
Why is a single train-test split misleading sometimes?
\subsubsection*{Solution}
\begin{itemize}
  \item Performance depends on split; CV reduces variance.
\end{itemize}

\subsection*{Mini Demo (Python)}
Run from the lecture folder:
\begin{verbatim}
python demo/demo.py
\end{verbatim}

Output files:
\begin{itemize}
  \item \texttt{images/demo.png}
  \item \texttt{data/results.txt}
\end{itemize}
\paragraph{What to show and say.}
\begin{itemize}
  \item Runs a small CV-like loop and compares model settings across folds.
  \item Shows why tuning on the test set is leakage and gives optimistic results.
  \item Use it to explain grid size and why we use pipelines.
\end{itemize}

\subsection*{Demo Output (Example)}
\begin{center}
\IfFileExists{../images/demo.png}{
  \includegraphics[width=0.95\linewidth]{../images/demo.png}
}{
  \small (Run the demo to generate \texttt{images/demo.png})
}
\end{center}

\subsection*{Summary}
\begin{itemize}
  \item Key definitions and the main formula.
  \item How to interpret results in context.
  \item How the demo connects to the theory.
\end{itemize}

\subsection*{Exit Question}
Why must you never use the test set to choose hyperparameters?
\paragraph{Suggested answer (for revision).}
Using the test set for tuning leaks information and makes performance look better than it
will be on truly new data; keep test set untouched.

\section*{References}
\begin{itemize}
  \item Montgomery, D. C., \& Runger, G. C. \textit{Applied Statistics and Probability for Engineers}, Wiley.
  \item Devore, J. L. \textit{Probability and Statistics for Engineering and the Sciences}, Cengage.
  \item McKinney, W. \textit{Python for Data Analysis}, O'Reilly.
\end{itemize}

% BEGIN SLIDE APPENDIX (AUTO-GENERATED)
\clearpage
\section*{Appendix: Slide Deck Content (Reference)}
\noindent The material below is a reference copy of the slide deck content. Exercise solutions are explained in the main notes where applicable.

\subsection*{Title Slide}
\titlepage
        \vspace{-0.5em}
        \begin{center}
          \small \texttt{https://github.com/tali7c/Statistics-and-Data-Analysis}
        \end{center}
\subsection*{Quick Links}
\centering
        \textbf{Overview}\hspace{0.6em}
\textbf{Cross-validation}\hspace{0.6em}
\textbf{Hyper-parameter Tuning}\hspace{0.6em}
\textbf{Exercises}\hspace{0.6em}
\textbf{Demo}\hspace{0.6em}
\textbf{Summary}\hspace{0.6em}
\subsection*{Agenda}
\begin{itemize}
  \item Overview
  \item Cross-validation
  \item Hyper-parameter Tuning
  \item Exercises
  \item Demo
  \item Summary
\end{itemize}
\subsection*{Learning Outcomes}
\begin{itemize}
        \item Explain train/validation/test split roles
\item Describe k-fold cross-validation
\item Explain grid search vs random search
\item Avoid data leakage using pipelines
      \end{itemize}
\subsection*{Cross-validation: Key Points}
\begin{itemize}
        \item CV estimates generalization more stably than one split
\item k-fold repeats train/validate across folds
\item Average score guides selection
      \end{itemize}
\subsection*{Hyper-parameter Tuning: Key Points}
\begin{itemize}
        \item Grid search tries all combos
\item Random search samples combos efficiently
\item Never tune on the test set
      \end{itemize}
\subsection*{Exercise 1: Grid size}
\small
  3 parameters with 4 values each: how many combinations?
\subsection*{Solution 1}
\begin{itemize}
    \item $4^3 = 64$
  \end{itemize}
\subsection*{Exercise 2: Leakage}
\small
  Is scaling on full dataset before split leakage?
\subsection*{Solution 2}
\begin{itemize}
    \item Yes; fit preprocessing on training only.
  \end{itemize}
\subsection*{Exercise 3: Why CV}
\small
  Why is a single train-test split misleading sometimes?
\subsection*{Solution 3}
\begin{itemize}
    \item Performance depends on split; CV reduces variance.
  \end{itemize}
\subsection*{Mini Demo (Python)}
Run from the lecture folder:
  \begin{center}
    \texttt{python demo/demo.py}
  \end{center}
  \vspace{0.4em}
  Outputs:
  \begin{itemize}
    \item \texttt{images/demo.png}
    \item \texttt{data/results.txt}
  \end{itemize}
\subsection*{Demo Output (Example)}
\begin{center}
  \IfFileExists{../images/demo.png}{
    \includegraphics[width=0.92\linewidth]{demo.png}
  }{
    \small (Run demo to generate: \texttt{demo.png})
  }
  \end{center}
\subsection*{Summary}
\begin{itemize}
        \item Key definitions and the main formula.
\item How to interpret results in context.
\item How the demo connects to the theory.
      \end{itemize}
\subsection*{Exit Question}
\small
  Why must you never use the test set to choose hyperparameters?
% END SLIDE APPENDIX (AUTO-GENERATED)

\end{document}
