\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}
\geometry{margin=1in}

\title{Statistics and Data Analysis\\Unit 04 -- Lecture 07 Notes}
\author{Tofik Ali}
\date{\today}

\begin{document}
\maketitle

\section*{Topic}
Regularization continuation: bias-variance intuition, scaling, and choosing lambda (conceptually).

\subsection*{Learning Outcomes}
\begin{itemize}
  \item Compute and interpret VIF (basic)
  \item Explain AIC/BIC as model selection criteria (intuition)
  \item Write ridge and lasso objectives
  \item Explain coefficient shrinkage and feature selection idea
\end{itemize}

\section*{Detailed Notes}
These notes are designed to be read alongside the slides. They expand each slide bullet into
plain-language explanations, small worked examples, and common pitfalls. When a formula
appears, emphasize (1) what each symbol means, (2) the assumptions needed to use it, and (3)
how to interpret the final number in the problem context.

\section*{VIF}
\begin{itemize}
  \item Definition: $\mathrm{VIF}_j = 1/(1-R_j^2)$
  \item Higher VIF -> more multicollinearity
  \item Rule of thumb thresholds (5/10)
\end{itemize}

\section*{Ridge/Lasso}
\begin{itemize}
  \item Ridge uses L2 penalty (shrinks)
  \item Lasso uses L1 penalty (can set some to 0)
  \item Scale features before regularization
\end{itemize}

\section*{Exercises (with Solutions)}
\subsection*{Exercise 1: Compute VIF}
If $R_j^2=0.9$, compute $\mathrm{VIF}_j$.
\subsection*{Solution}
\begin{itemize}
  \item $\mathrm{VIF}_j = 1/(1-0.9)=10$ (high).
\end{itemize}

\subsection*{Exercise 2: Ridge vs lasso}
Which can produce exact zero coefficients?
\subsection*{Solution}
\begin{itemize}
  \item Lasso (L1) can set some coefficients to 0.
\end{itemize}

\subsection*{Exercise 3: AIC/BIC meaning}
Lower AIC/BIC means what (conceptually)?
\subsection*{Solution}
\begin{itemize}
  \item Better trade-off between fit and complexity (relative).
\end{itemize}

\section*{Exit Question}
Why can ridge help when predictors are highly correlated?

\section*{Demo (Python)}
Run from the lecture folder:
\begin{verbatim}
python demo/demo.py
\end{verbatim}

Output files:
\begin{itemize}
  \item \texttt{images/demo.png}
  \item \texttt{data/results.txt}
\end{itemize}

\section*{References}
\begin{itemize}
  \item Montgomery, D. C., \& Runger, G. C. \textit{Applied Statistics and Probability for Engineers}, Wiley.
  \item Devore, J. L. \textit{Probability and Statistics for Engineering and the Sciences}, Cengage.
  \item McKinney, W. \textit{Python for Data Analysis}, O'Reilly.
\end{itemize}
\end{document}
