\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}
\geometry{margin=1in}

\title{Statistics and Data Analysis\\Unit 02 -- Lecture 03 Notes\\Correlation, Skewness, Kurtosis}
\author{Tofik Ali}
\date{\today}

\begin{document}
\maketitle

\section*{What You Will Learn (Beginner-Friendly)}
This lecture answers two practical questions:
\begin{enumerate}
  \item If two variables (like study hours and score) move together, how do we measure the \emph{strength} of that relationship?
  \item If we look at one variable (like income), how do we describe the \emph{shape} of its distribution beyond center and spread?
\end{enumerate}

By the end, you should be able to:
\begin{itemize}
  \item Define \textbf{Pearson correlation} and compute it on small paired datasets.
  \item Explain why correlation is \textbf{scale-free} and how it relates to covariance.
  \item Explain key cautions: \textbf{outliers}, \textbf{non-linearity}, and \textbf{correlation vs causation}.
  \item Interpret \textbf{skewness} (right/left skew) and \textbf{kurtosis} (tail heaviness).
  \item Compute simple \textbf{moment skewness} and \textbf{excess kurtosis} for small datasets.
\end{itemize}

\section*{1. Correlation}

\subsection*{1.1 Intuition}
Correlation measures \textbf{linear association} between two variables.
If a scatter plot looks like an upward sloping line, correlation is positive.
If it looks like a downward sloping line, correlation is negative.

\subsection*{1.2 Pearson correlation formula}
For paired observations $(x_i,y_i)$, the Pearson correlation is:
\[
r = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}
         {\sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i-\bar{y})^2}}
\]
Key facts:
\begin{itemize}
  \item $-1 \le r \le 1$.
  \item The sign ($+$ or $-$) shows direction (increase together vs opposite).
  \item The magnitude $|r|$ shows strength of \emph{linear} relationship.
  \item Correlation is \textbf{unitless} (scale-free).
\end{itemize}

\subsection*{1.3 Correlation vs covariance}
In Lecture 02 we computed \textbf{sample covariance}:
\[
s_{xy}=\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})
\]
Correlation is the standardized version:
\[
r=\frac{s_{xy}}{s_x s_y}
\]
where $s_x$ and $s_y$ are sample standard deviations.

\paragraph{Why ``scale-free'' matters.}
If we change units (hours to minutes, marks to percentage), covariance changes because its units change.
But correlation does \emph{not} change, because the scaling affects the numerator and denominator in the same way.

\subsection*{1.4 Common pitfalls}
\begin{itemize}
  \item \textbf{Outliers:} a single extreme point can strongly change $r$.
  \item \textbf{Non-linearity:} you can have a strong relationship but $r \approx 0$ if the pattern is curved.
  \item \textbf{Correlation vs causation:} even a high correlation does not prove that $x$ causes $y$.
\end{itemize}

\section*{2. Exercises (Correlation)}

\subsection*{Exercise 1: Pearson correlation (positive)}
Hours studied vs Score:
\[
x=[1,2,3,4,5],\quad y=[52,55,60,65,68]
\]
Given: $\bar{x}=3$, $\bar{y}=60$.

\paragraph{Step 1: Compute deviations and sums.}
\[
x-\bar{x}=[-2,-1,0,1,2]
\]
\[
y-\bar{y}=[-8,-5,0,5,8]
\]
Now compute:
\[
\sum (x-\bar{x})(y-\bar{y}) = (16+5+0+5+16)=42
\]
\[
\sum (x-\bar{x})^2 = 4+1+0+1+4=10
\]
\[
\sum (y-\bar{y})^2 = 64+25+0+25+64=178
\]

\paragraph{Step 2: Plug into Pearson formula.}
\[
r=\frac{42}{\sqrt{10}\sqrt{178}}=\frac{42}{\sqrt{1780}}\approx 0.9955
\]
\textbf{Interpretation:} Very strong positive linear association between hours studied and score.

\subsection*{Exercise 2: Pearson correlation (negative)}
Price vs Demand:
\[
x=[1,2,3,4,5],\quad y=[80,70,60,50,40]
\]
Here the relationship is perfectly linear: $y=90-10x$.
So:
\[
r=-1
\]
\textbf{Interpretation:} Perfect negative linear relationship.

\subsection*{Exercise 3: $r=0$ but strong relationship}
Let:
\[
x=[-2,-1,0,1,2],\quad y=x^2=[4,1,0,1,4]
\]
Compute means:
\[
\bar{x}=0,\quad \bar{y}=\frac{4+1+0+1+4}{5}=2
\]
Compute numerator:
\[
\sum (x-\bar{x})(y-\bar{y}) = (-2)(2)+(-1)(-1)+0(-2)+1(-1)+2(2)=0
\]
So:
\[
r=0
\]
\textbf{Interpretation:} No \emph{linear} association, but a strong \emph{non-linear} relationship exists ($y$ is determined by $x$).

\subsection*{Exercise 4: Correlation vs causation}
Statement: ``Ice cream sales and drowning incidents are positively correlated.''

\textbf{Best explanation:} A third variable like temperature/season can increase both ice cream sales and swimming activity.
This is called \textbf{confounding}. Correlation alone cannot prove causation.

\section*{3. Skewness}

\subsection*{3.1 What skewness means}
Skewness describes \textbf{asymmetry} in a distribution:
\begin{itemize}
  \item \textbf{Right-skew (positive skew):} long tail to the right; a few very large values.
  \item \textbf{Left-skew (negative skew):} long tail to the left; a few very small values.
\end{itemize}

\subsection*{3.2 Mean vs median (important intuition)}
The mean is pulled toward extreme values more than the median.
So:
\begin{itemize}
  \item Right-skewed: mean $>$ median (high outliers pull mean up).
  \item Left-skewed: mean $<$ median (low outliers pull mean down).
\end{itemize}

\subsection*{3.3 Moment skewness (one common definition)}
Define central moments (divide by $n$):
\[
m_k=\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar{x})^k
\]
Moment skewness:
\[
g_1=\frac{m_3}{m_2^{3/2}}
\]
Notes:
\begin{itemize}
  \item $g_1>0$ indicates right skew; $g_1<0$ indicates left skew.
  \item Some software uses bias-corrected formulas; values can differ slightly.
\end{itemize}

\section*{4. Exercises (Skewness)}

\subsection*{Exercise 5: Identify skewness direction using mean/median}
Dataset A (Income, INR thousands): 20, 22, 23, 24, 25, 26, 27, 28, 60.

Dataset B (Scores): 50, 80, 85, 88, 90, 92, 93, 94, 95, 96.

\paragraph{Dataset A.}
Median is 25 (middle value). Mean is:
\[
\bar{x}=\frac{20+22+23+24+25+26+27+28+60}{9}=\frac{255}{9}\approx 28.33
\]
Since mean $>$ median, the data is right-skewed (positive skew).

\paragraph{Dataset B.}
Median is $(90+92)/2=91$. Mean is:
\[
\bar{x}=\frac{50+80+85+88+90+92+93+94+95+96}{10}=\frac{863}{10}=86.3
\]
Since mean $<$ median, the data is left-skewed (negative skew).

\subsection*{Exercise 6: Compute moment skewness (income)}
Suppose for Dataset A (income) we have:
\[
\bar{x}=28.33,\quad m_2=130.89,\quad m_3=3404.07
\]
Then:
\[
g_1=\frac{3404.07}{(130.89)^{3/2}}\approx 2.27
\]
\textbf{Interpretation:} strongly right-skewed distribution.

\section*{5. Kurtosis}

\subsection*{5.1 What kurtosis means}
Kurtosis is commonly used to describe \textbf{tail heaviness} (how often extreme values occur).
It is often reported as \textbf{excess kurtosis}:
\[
\text{Excess} = \text{kurtosis} - 3
\]
The normal distribution has excess kurtosis 0.

\subsection*{5.2 Moment kurtosis (one common definition)}
Moment kurtosis:
\[
g_2=\frac{m_4}{m_2^2}
\]
Excess kurtosis:
\[
g_2-3
\]
Interpretation:
\begin{itemize}
  \item Excess $>0$: heavier tails (more extreme values than normal).
  \item Excess $<0$: lighter tails (fewer extremes than normal).
\end{itemize}

\section*{6. Exercises (Kurtosis)}

\subsection*{Exercise 7: Excess kurtosis for 1,2,3,4,5}
Dataset: 1, 2, 3, 4, 5. Mean is 3. Deviations: $[-2,-1,0,1,2]$.

\paragraph{Step 1: Compute $m_2$.}
\[
m_2=\frac{4+1+0+1+4}{5}=2
\]
\paragraph{Step 2: Compute $m_4$.}
\[
m_4=\frac{16+1+0+1+16}{5}=\frac{34}{5}=6.8
\]
\paragraph{Step 3: Compute kurtosis and excess.}
\[
g_2=\frac{6.8}{2^2}=1.7,\quad \text{excess}=1.7-3=-1.3
\]
\textbf{Interpretation:} negative excess kurtosis (lighter tails than normal).

\subsection*{Exercise 8: Excess kurtosis for the income example}
Suppose for the income dataset:
\[
m_2=130.89,\quad m_4=112590.30
\]
Then:
\[
g_2=\frac{112590.30}{(130.89)^2}\approx 6.57,\quad \text{excess}\approx 3.57
\]
\textbf{Interpretation:} large positive excess kurtosis indicates heavy tails / extreme values (outliers).

\section*{7. Mini Demo (Python)}
Run this from the lecture folder:
\begin{verbatim}
python demo/correlation_skew_kurt_demo.py
\end{verbatim}

The script:
\begin{itemize}
  \item computes Pearson correlation for:
    \begin{itemize}
      \item hours vs score (positive)
      \item price vs demand (negative)
      \item $x$ vs $x^2$ (non-linear example)
    \end{itemize}
  \item prints correlation values among features in \texttt{data/student\_metrics.csv}
  \item computes moment skewness and excess kurtosis for example univariate datasets
  \item optionally saves plots into \texttt{images/} if \texttt{matplotlib} is installed
\end{itemize}

\section*{References}
\begin{itemize}
  \item Montgomery, D. C., \& Runger, G. C. \textit{Applied Statistics and Probability for Engineers}, Wiley, 7th ed., 2020.
  \item Gupta, S. C., \& Kapoor, V. K. \textit{Fundamentals of Applied Statistics}, Sultan Chand \& Sons, 4th rev. ed., 2007.
  \item McKinney, W. \textit{Python for Data Analysis}, O'Reilly, 2022.
\end{itemize}

\end{document}
