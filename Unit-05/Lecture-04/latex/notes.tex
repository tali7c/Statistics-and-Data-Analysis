\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\graphicspath{{../images/}}
\usepackage{hyperref}
\geometry{margin=1in}

\title{Statistics and Data Analysis\\Unit 05 -- Lecture 04 Notes\\Factor Analysis and Discriminant Analysis (LDA)}
\author{Tofik Ali}
\date{\today}

\begin{document}
\maketitle

\section*{Topic}
Factor analysis (latent factors) and LDA (supervised separation).
\section*{How to Use These Notes}
These notes are written for students who are seeing the topic for the first time. They
follow the slide order, but add the missing 'why', interpretation, and common mistakes. If
you get stuck, look at the worked exercises and then run the Python demo.

Course repository (slides, demos, datasets): \url{https://github.com/tali7c/Statistics-and-Data-Analysis}

\section*{Time Plan (55 minutes)}
\begin{itemize}
  \item 0--10 min: Attendance + recap of previous lecture
  \item 10--35 min: Core concepts (this lecture's sections)
  \item 35--45 min: Exercises (solve 1--2 in class, rest as practice)
  \item 45--50 min: Mini demo + interpretation of output
  \item 50--55 min: Buffer / wrap-up (leave 5 minutes early)
\end{itemize}

\section*{Slide-by-slide Notes}
\subsection*{Title Slide}
State the lecture title clearly and connect it to what students already know.
Tell students what they will be able to do by the end (not just what you will cover).

\subsection*{Quick Links / Agenda}
Explain the structure of the lecture and where the exercises and demo appear.
\begin{itemize}
  \item Overview
  \item Factor Analysis
  \item LDA
  \item Exercises
  \item Demo
  \item Summary
\end{itemize}

\subsection*{Learning Outcomes}
\begin{itemize}
  \item Explain factor analysis as latent-factor modeling (intuition)
  \item Differentiate PCA vs factor analysis (goal/assumptions)
  \item Explain LDA as supervised dimensionality reduction/classifier
  \item Interpret a 2D LDA projection
\end{itemize}
\paragraph{Why these outcomes matter.}
Always state assumptions clearly. Common assumptions in classical tests include independence
of observations, roughly normal errors (or a large-sample justification), and similar
variances across groups. Violations do not automatically invalidate a result, but they
change how much you should trust the p-value and confidence interval.
\textbf{PCA} finds new axes (principal components) that capture maximum variance. It is a
rotation of the feature space. Because PCA is variance-based, it is sensitive to scaling:
standardize features first unless all features are already comparable.

\subsection*{Factor Analysis: Key Points}
\begin{itemize}
  \item Observed variables driven by a few latent factors
  \item Goal: explain correlations via factors
  \item Used for surveys/constructs
\end{itemize}
\paragraph{Explanation.}
\textbf{Correlation} measures the strength of a linear association between two variables. It
is symmetric (no X/Y direction) and does not imply causation. Outliers can inflate or hide
correlation, so always look at the scatter plot.

\subsection*{LDA: Key Points}
\begin{itemize}
  \item Supervised: uses labels
  \item Finds projection maximizing class separation
  \item Can classify and visualize
\end{itemize}

\subsection*{Exercises (with Solutions)}
Attempt the exercise first, then compare with the solution. Focus on interpretation, not
only arithmetic.

\subsection*{Exercise 1: Supervised?}
Is PCA supervised? Is LDA supervised?
\subsubsection*{Solution}
\begin{itemize}
  \item PCA is unsupervised; LDA is supervised.
\end{itemize}
\paragraph{Walkthrough.}
\textbf{PCA} finds new axes (principal components) that capture maximum variance. It is a
rotation of the feature space. Because PCA is variance-based, it is sensitive to scaling:
standardize features first unless all features are already comparable.

\subsection*{Exercise 2: Goal}
What does PCA optimize vs LDA (intuition)?
\subsubsection*{Solution}
\begin{itemize}
  \item PCA: variance captured; LDA: class separability.
\end{itemize}
\paragraph{Walkthrough.}
\textbf{PCA} finds new axes (principal components) that capture maximum variance. It is a
rotation of the feature space. Because PCA is variance-based, it is sensitive to scaling:
standardize features first unless all features are already comparable.

\subsection*{Exercise 3: Use case}
Labeled A/B/C data, want 2D plot separating classes. PCA or LDA?
\subsubsection*{Solution}
\begin{itemize}
  \item LDA (uses labels for separation).
\end{itemize}
\paragraph{Walkthrough.}
\textbf{PCA} finds new axes (principal components) that capture maximum variance. It is a
rotation of the feature space. Because PCA is variance-based, it is sensitive to scaling:
standardize features first unless all features are already comparable.

\subsection*{Mini Demo (Python)}
Run from the lecture folder:
\begin{verbatim}
python demo/demo.py
\end{verbatim}

Output files:
\begin{itemize}
  \item \texttt{images/demo.png}
  \item \texttt{data/results.txt}
\end{itemize}
\paragraph{What to show and say.}
\begin{itemize}
  \item Illustrates factor-like structure and an LDA-style supervised projection.
  \item Use it to compare 'variance capture' (PCA) vs 'class separation' (LDA).
  \item Discuss when interpretability vs prediction is the goal.
\end{itemize}

\subsection*{Demo Output (Example)}
\begin{center}
\IfFileExists{../images/demo.png}{
  \includegraphics[width=0.95\linewidth]{../images/demo.png}
}{
  \small (Run the demo to generate \texttt{images/demo.png})
}
\end{center}

\subsection*{Summary}
\begin{itemize}
  \item Key definitions and the main formula.
  \item How to interpret results in context.
  \item How the demo connects to the theory.
\end{itemize}

\subsection*{Exit Question}
Why can LDA separate classes better than PCA on labeled data?
\paragraph{Suggested answer (for revision).}
LDA uses labels to maximize class separation, while PCA ignores labels and only maximizes
variance, so LDA can separate classes better.

\section*{References}
\begin{itemize}
  \item Montgomery, D. C., \& Runger, G. C. \textit{Applied Statistics and Probability for Engineers}, Wiley.
  \item Devore, J. L. \textit{Probability and Statistics for Engineering and the Sciences}, Cengage.
  \item McKinney, W. \textit{Python for Data Analysis}, O'Reilly.
\end{itemize}

% BEGIN SLIDE APPENDIX (AUTO-GENERATED)
\clearpage
\section*{Appendix: Slide Deck Content (Reference)}
\noindent The material below is a reference copy of the slide deck content. Exercise solutions are explained in the main notes where applicable.

\subsection*{Title Slide}
\titlepage
        \vspace{-0.5em}
        \begin{center}
          \small \texttt{https://github.com/tali7c/Statistics-and-Data-Analysis}
        \end{center}
\subsection*{Quick Links}
\centering
        \textbf{Overview}\hspace{0.6em}
\textbf{Factor Analysis}\hspace{0.6em}
\textbf{LDA}\hspace{0.6em}
\textbf{Exercises}\hspace{0.6em}
\textbf{Demo}\hspace{0.6em}
\textbf{Summary}\hspace{0.6em}
\subsection*{Agenda}
\begin{itemize}
  \item Overview
  \item Factor Analysis
  \item LDA
  \item Exercises
  \item Demo
  \item Summary
\end{itemize}
\subsection*{Learning Outcomes}
\begin{itemize}
        \item Explain factor analysis as latent-factor modeling (intuition)
\item Differentiate PCA vs factor analysis (goal/assumptions)
\item Explain LDA as supervised dimensionality reduction/classifier
\item Interpret a 2D LDA projection
      \end{itemize}
\subsection*{Factor Analysis: Key Points}
\begin{itemize}
        \item Observed variables driven by a few latent factors
\item Goal: explain correlations via factors
\item Used for surveys/constructs
      \end{itemize}
\subsection*{LDA: Key Points}
\begin{itemize}
        \item Supervised: uses labels
\item Finds projection maximizing class separation
\item Can classify and visualize
      \end{itemize}
\subsection*{Exercise 1: Supervised?}
\small
  Is PCA supervised? Is LDA supervised?
\subsection*{Solution 1}
\begin{itemize}
    \item PCA is unsupervised; LDA is supervised.
  \end{itemize}
\subsection*{Exercise 2: Goal}
\small
  What does PCA optimize vs LDA (intuition)?
\subsection*{Solution 2}
\begin{itemize}
    \item PCA: variance captured; LDA: class separability.
  \end{itemize}
\subsection*{Exercise 3: Use case}
\small
  Labeled A/B/C data, want 2D plot separating classes. PCA or LDA?
\subsection*{Solution 3}
\begin{itemize}
    \item LDA (uses labels for separation).
  \end{itemize}
\subsection*{Mini Demo (Python)}
Run from the lecture folder:
  \begin{center}
    \texttt{python demo/demo.py}
  \end{center}
  \vspace{0.4em}
  Outputs:
  \begin{itemize}
    \item \texttt{images/demo.png}
    \item \texttt{data/results.txt}
  \end{itemize}
\subsection*{Demo Output (Example)}
\begin{center}
  \IfFileExists{../images/demo.png}{
    \includegraphics[width=0.92\linewidth]{demo.png}
  }{
    \small (Run demo to generate: \texttt{demo.png})
  }
  \end{center}
\subsection*{Summary}
\begin{itemize}
        \item Key definitions and the main formula.
\item How to interpret results in context.
\item How the demo connects to the theory.
      \end{itemize}
\subsection*{Exit Question}
\small
  Why can LDA separate classes better than PCA on labeled data?
% END SLIDE APPENDIX (AUTO-GENERATED)

\end{document}
