\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\graphicspath{{../images/}}
\usepackage{hyperref}
\geometry{margin=1in}

\title{Statistics and Data Analysis\\Unit 05 -- Lecture 03 Notes\\Principal Component Analysis (PCA)}
\author{Tofik Ali}
\date{\today}

\begin{document}
\maketitle

\section*{Topic}
PCA: variance-maximizing projection; explained variance; scaling.
\section*{How to Use These Notes}
These notes are written for students who are seeing the topic for the first time. They
follow the slide order, but add the missing 'why', interpretation, and common mistakes. If
you get stuck, look at the worked exercises and then run the Python demo.

Course repository (slides, demos, datasets): \url{https://github.com/tali7c/Statistics-and-Data-Analysis}

\section*{Time Plan (55 minutes)}
\begin{itemize}
  \item 0--10 min: Attendance + recap of previous lecture
  \item 10--35 min: Core concepts (this lecture's sections)
  \item 35--45 min: Exercises (solve 1--2 in class, rest as practice)
  \item 45--50 min: Mini demo + interpretation of output
  \item 50--55 min: Buffer / wrap-up (leave 5 minutes early)
\end{itemize}

\section*{Slide-by-slide Notes}
\subsection*{Title Slide}
State the lecture title clearly and connect it to what students already know.
Tell students what they will be able to do by the end (not just what you will cover).

\subsection*{Quick Links / Agenda}
Explain the structure of the lecture and where the exercises and demo appear.
\begin{itemize}
  \item Overview
  \item PCA Intuition
  \item Explained Variance
  \item Exercises
  \item Demo
  \item Summary
\end{itemize}

\subsection*{Learning Outcomes}
\begin{itemize}
  \item Explain PCA as a variance-maximizing linear projection
  \item State why scaling is important before PCA
  \item Interpret explained variance ratio and scree plot
  \item Use PCA for visualization and noise reduction
\end{itemize}
\paragraph{Why these outcomes matter.}
\textbf{PCA} finds new axes (principal components) that capture maximum variance. It is a
rotation of the feature space. Because PCA is variance-based, it is sensitive to scaling:
standardize features first unless all features are already comparable.
\textbf{Explained variance} tells how much of the total variance is captured by the first
$k$ principal components. Use it to choose the number of components: keep enough to capture
most variance while still reducing dimensionality.

\subsection*{PCA Intuition: Key Points}
\begin{itemize}
  \item Find new axes (components) capturing maximum variance
  \item Components are orthogonal
  \item PC1 captures most variance
\end{itemize}
\paragraph{Explanation.}
\textbf{PCA} finds new axes (principal components) that capture maximum variance. It is a
rotation of the feature space. Because PCA is variance-based, it is sensitive to scaling:
standardize features first unless all features are already comparable.

\subsection*{Explained Variance: Key Points}
\begin{itemize}
  \item Explained variance ratio per component
  \item Choose k via scree plot / cumulative variance target
  \item Validate downstream performance
\end{itemize}
\paragraph{Explanation.}
\textbf{Explained variance} tells how much of the total variance is captured by the first
$k$ principal components. Use it to choose the number of components: keep enough to capture
most variance while still reducing dimensionality.

\subsection*{Exercises (with Solutions)}
Attempt the exercise first, then compare with the solution. Focus on interpretation, not
only arithmetic.

\subsection*{Exercise 1: Scaling}
Why scale features before PCA?
\subsubsection*{Solution}
\begin{itemize}
  \item To prevent large-unit features dominating variance.
\end{itemize}
\paragraph{Walkthrough.}
\textbf{PCA} finds new axes (principal components) that capture maximum variance. It is a
rotation of the feature space. Because PCA is variance-based, it is sensitive to scaling:
standardize features first unless all features are already comparable.

\subsection*{Exercise 2: Components}
Are PCA components original features?
\subsubsection*{Solution}
\begin{itemize}
  \item No; they are linear combinations.
\end{itemize}
\paragraph{Walkthrough.}
\textbf{PCA} finds new axes (principal components) that capture maximum variance. It is a
rotation of the feature space. Because PCA is variance-based, it is sensitive to scaling:
standardize features first unless all features are already comparable.

\subsection*{Exercise 3: Choosing k}
If first 2 PCs explain 88\% and you need 90\%, what do you do?
\subsubsection*{Solution}
\begin{itemize}
  \item Add next PC(s) until target reached.
\end{itemize}

\subsection*{Mini Demo (Python)}
Run from the lecture folder:
\begin{verbatim}
python demo/demo.py
\end{verbatim}

Output files:
\begin{itemize}
  \item \texttt{images/demo.png}
  \item \texttt{data/results.txt}
\end{itemize}
\paragraph{What to show and say.}
\begin{itemize}
  \item Runs PCA on correlated features and plots the 2D projection.
  \item Reports explained variance ratios to discuss choosing k.
  \item Use it to show why scaling changes PCA results dramatically.
\end{itemize}

\subsection*{Demo Output (Example)}
\begin{center}
\IfFileExists{../images/demo.png}{
  \includegraphics[width=0.95\linewidth]{../images/demo.png}
}{
  \small (Run the demo to generate \texttt{images/demo.png})
}
\end{center}

\subsection*{Summary}
\begin{itemize}
  \item Key definitions and the main formula.
  \item How to interpret results in context.
  \item How the demo connects to the theory.
\end{itemize}

\subsection*{Exit Question}
Why might PCA improve a model even though it discards some variance?
\paragraph{Suggested answer (for revision).}
PCA can remove noise/redundancy; discarding low-variance directions can improve
generalization if those directions are mostly noise.

\section*{References}
\begin{itemize}
  \item Montgomery, D. C., \& Runger, G. C. \textit{Applied Statistics and Probability for Engineers}, Wiley.
  \item Devore, J. L. \textit{Probability and Statistics for Engineering and the Sciences}, Cengage.
  \item McKinney, W. \textit{Python for Data Analysis}, O'Reilly.
\end{itemize}

% BEGIN SLIDE APPENDIX (AUTO-GENERATED)
\clearpage
\section*{Appendix: Slide Deck Content (Reference)}
\noindent The material below is a reference copy of the slide deck content. Exercise solutions are explained in the main notes where applicable.

\subsection*{Title Slide}
\titlepage
        \vspace{-0.5em}
        \begin{center}
          \small \texttt{https://github.com/tali7c/Statistics-and-Data-Analysis}
        \end{center}
\subsection*{Quick Links}
\centering
        \textbf{Overview}\hspace{0.6em}
\textbf{PCA Intuition}\hspace{0.6em}
\textbf{Explained Variance}\hspace{0.6em}
\textbf{Exercises}\hspace{0.6em}
\textbf{Demo}\hspace{0.6em}
\textbf{Summary}\hspace{0.6em}
\subsection*{Agenda}
\begin{itemize}
  \item Overview
  \item PCA Intuition
  \item Explained Variance
  \item Exercises
  \item Demo
  \item Summary
\end{itemize}
\subsection*{Learning Outcomes}
\begin{itemize}
        \item Explain PCA as a variance-maximizing linear projection
\item State why scaling is important before PCA
\item Interpret explained variance ratio and scree plot
\item Use PCA for visualization and noise reduction
      \end{itemize}
\subsection*{PCA Intuition: Key Points}
\begin{itemize}
        \item Find new axes (components) capturing maximum variance
\item Components are orthogonal
\item PC1 captures most variance
      \end{itemize}
\subsection*{Explained Variance: Key Points}
\begin{itemize}
        \item Explained variance ratio per component
\item Choose k via scree plot / cumulative variance target
\item Validate downstream performance
      \end{itemize}
\subsection*{Exercise 1: Scaling}
\small
  Why scale features before PCA?
\subsection*{Solution 1}
\begin{itemize}
    \item To prevent large-unit features dominating variance.
  \end{itemize}
\subsection*{Exercise 2: Components}
\small
  Are PCA components original features?
\subsection*{Solution 2}
\begin{itemize}
    \item No; they are linear combinations.
  \end{itemize}
\subsection*{Exercise 3: Choosing k}
\small
  If first 2 PCs explain 88\% and you need 90\%, what do you do?
\subsection*{Solution 3}
\begin{itemize}
    \item Add next PC(s) until target reached.
  \end{itemize}
\subsection*{Mini Demo (Python)}
Run from the lecture folder:
  \begin{center}
    \texttt{python demo/demo.py}
  \end{center}
  \vspace{0.4em}
  Outputs:
  \begin{itemize}
    \item \texttt{images/demo.png}
    \item \texttt{data/results.txt}
  \end{itemize}
\subsection*{Demo Output (Example)}
\begin{center}
  \IfFileExists{../images/demo.png}{
    \includegraphics[width=0.92\linewidth]{demo.png}
  }{
    \small (Run demo to generate: \texttt{demo.png})
  }
  \end{center}
\subsection*{Summary}
\begin{itemize}
        \item Key definitions and the main formula.
\item How to interpret results in context.
\item How the demo connects to the theory.
      \end{itemize}
\subsection*{Exit Question}
\small
  Why might PCA improve a model even though it discards some variance?
% END SLIDE APPENDIX (AUTO-GENERATED)

\end{document}
