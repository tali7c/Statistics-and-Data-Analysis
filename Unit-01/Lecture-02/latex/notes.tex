\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}
\geometry{margin=1in}

\title{Statistics and Data Analysis\\Unit 01 -- Lecture 02 Notes\\Feature Scaling and Feature Engineering Basics}
\author{Tofik Ali}
\date{\today}

\begin{document}
\maketitle

\section*{What You Will Learn}
In this lecture we focus on practical preprocessing steps that are used \emph{before} modeling:
\begin{itemize}
  \item scaling (normalization and standardization),
  \item transformations (especially for skewed variables),
  \item and basic feature engineering (creating helpful new variables).
\end{itemize}

\section*{1. Feature Scaling}

\subsection*{1.1 Why scaling is needed}
Many datasets contain features with different units and magnitudes, for example:
\begin{itemize}
  \item attendance (\%) typically ranges 0--100,
  \item study hours per week might range 0--25,
  \item income could range 20--200 (in thousands) or much more.
\end{itemize}

If we use distance (like Euclidean distance), the largest-scale feature can dominate. This is why scaling is important for:
\begin{itemize}
  \item kNN (nearest neighbors),
  \item k-means clustering,
  \item many gradient-based optimization methods.
\end{itemize}

\subsection*{1.2 Min--max normalization}
Min--max scaling maps a feature into $[0,1]$:
\[
x' = \frac{x-\min(x)}{\max(x)-\min(x)}
\]
Interpretation: $x'=0$ means the minimum observed value; $x'=1$ means the maximum observed value.

\paragraph{Exercise 1 (solution).}
If min=20, max=80, and $x=50$:
\[
x'=\frac{50-20}{80-20}=\frac{30}{60}=0.5
\]

\subsection*{1.3 Standardization (z-score)}
Standardization uses mean and standard deviation:
\[
z=\frac{x-\mu}{\sigma}
\]
Interpretation: $z$ is the number of standard deviations $x$ is above/below the mean.

\paragraph{Exercise 2 (solution).}
If $\mu=60$, $\sigma=10$, and $x=80$:
\[
z=\frac{80-60}{10}=2
\]
So the score 80 is 2 standard deviations above the mean.

\subsection*{1.4 Important warning: data leakage}
When we scale, we compute parameters (min/max or mean/std). These parameters must be estimated on the \textbf{training data only}.
Then we use the same parameters to transform the test set.

If we use the whole dataset (including test) to compute scaling parameters, then test information ``leaks'' into training. That can make performance look better than it truly is.

\paragraph{Exercise 3 (solution).}
\begin{itemize}
  \item kNN: Yes (distance-based)
  \item k-means: Yes (distance-based)
  \item Linear regression (gradient descent): often yes (helps optimization)
  \item Decision tree: usually no (split rules are scale-invariant)
\end{itemize}

\section*{2. Transformations}

\subsection*{2.1 Why transform?}
Scaling changes the \emph{units} or spread, but it does not necessarily fix skewness.
For right-skewed features (like income), many values are small and a few values are extremely large.

Transformations can:
\begin{itemize}
  \item reduce skewness,
  \item make relationships more linear,
  \item reduce the impact of extreme values.
\end{itemize}

\subsection*{2.2 Log transform}
A common transform for positive values is:
\[
x'=\log(1+x)
\]
We use $\log(1+x)$ instead of $\log(x)$ to handle zero values safely.

\paragraph{Exercise 4 (solution).}
For right-skewed income, a reasonable first transform is $\log(1+x)$.

\section*{3. Feature Engineering Basics}

\subsection*{3.1 What is feature engineering?}
Feature engineering means creating new variables (features) from raw columns.
Good engineered features can:
\begin{itemize}
  \item capture patterns more directly,
  \item improve model performance,
  \item and make interpretations clearer.
\end{itemize}

\subsection*{3.2 Common patterns}
\begin{itemize}
  \item \textbf{Encoding categorical variables}: one-hot encoding for nominal categories.
  \item \textbf{Buckets/bins}: convert continuous variables to categories (e.g., attendance low/medium/high).
  \item \textbf{Interactions}: multiply/add features when combined effect matters (effort = hours $\times$ attendance).
  \item \textbf{Ratios}: e.g., marks per hour, cost per unit.
  \item \textbf{Date parts}: month, weekday, time-of-day.
  \item \textbf{Text features}: word count, length, keyword flags (basic features).
\end{itemize}

\paragraph{Exercise 5 (example answers).}
\begin{itemize}
  \item \texttt{income\_log1p}: reduces skew and outlier impact.
  \item \texttt{effort\_index} = hours $\times$ attendance/100: single ``effort'' score.
  \item \texttt{join\_month}: captures seasonal effects.
\end{itemize}

\paragraph{Exercise 6 (solution).}
If \texttt{program} has categories CSE/ECE/AIML, the one-hot columns can be:
\[
\texttt{program\_CSE},\ \texttt{program\_ECE},\ \texttt{program\_AIML}
\]
Each row has exactly one 1 (its category) and 0 for the others.

\section*{4. Mini Demo (Python)}
Run from the lecture folder:
\begin{verbatim}
python demo/scaling_feature_engineering_demo.py
\end{verbatim}

The demo:
\begin{itemize}
  \item scales three features (attendance, hours, income) and prints how distances change after scaling,
  \item engineers features (month, weekday, income log, effort index, backlog flag, one-hot program),
  \item saves \texttt{data/student\_features\_engineered.csv},
  \item saves plots in \texttt{images/}.
\end{itemize}

\section*{References}
\begin{itemize}
  \item McKinney, W. \textit{Python for Data Analysis}. O'Reilly, 2022.
  \item Kuhn, M., \& Johnson, K. \textit{Feature Engineering and Selection}. CRC Press, 2019.
  \item Montgomery, D. C., \& Runger, G. C. \textit{Applied Statistics and Probability for Engineers}. Wiley, 7th ed., 2020.
\end{itemize}

\end{document}

