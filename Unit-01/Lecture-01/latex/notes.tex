\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}
\geometry{margin=1in}

\title{Statistics and Data Analysis\\Unit 01 -- Lecture 01 Notes\\Data Types, Sources, and Cleaning Basics}
\author{Tofik Ali}
\date{\today}

\begin{document}
\maketitle

\section*{Why This Lecture Exists}
Before we compute statistics or build models, we must ensure the data is:
\begin{itemize}
  \item correctly \textbf{typed} (numbers are numbers, dates are dates),
  \item correctly \textbf{formatted} (consistent schema and representation),
  \item and reasonably \textbf{clean} (no obvious errors, duplicates, or impossible values).
\end{itemize}
Otherwise, we can get very convincing but completely wrong conclusions.

\section*{1. Dataset Basics}

\subsection*{1.1 Observation vs variable}
\begin{itemize}
  \item An \textbf{observation} is one record/row (e.g., one student).
  \item A \textbf{variable} (or feature/attribute) is one column (e.g., attendance\%).
  \item A \textbf{dataset} is a table of observations and variables.
\end{itemize}

\subsection*{1.2 Why type matters}
If a numeric column is stored as text, then:
\begin{itemize}
  \item sorting can become wrong (``100'' comes before ``20'' in string order),
  \item mean/median cannot be computed correctly,
  \item plots may fail or mislead.
\end{itemize}
So the first step in almost every analysis is: \textbf{inspect data types}.

\section*{2. Data Types and Formats}

\subsection*{2.1 Common data types (practical)}
\begin{itemize}
  \item \textbf{Numeric}: integers (count) and real values (measurement).
  \item \textbf{Categorical}:
    \begin{itemize}
      \item \textbf{Nominal}: no natural order (branch = CSE/ECE).
      \item \textbf{Ordinal}: ordered categories (rating = low/medium/high).
    \end{itemize}
  \item \textbf{Binary}: yes/no, 0/1, pass/fail.
  \item \textbf{Datetime}: dates and timestamps.
  \item \textbf{Text}: comments, feedback (often unstructured).
\end{itemize}

\subsection*{2.2 Data formats}
\begin{itemize}
  \item \textbf{Structured}: fixed schema, tabular (CSV, SQL tables).
  \item \textbf{Semi-structured}: key-value or tagged (JSON, XML).
  \item \textbf{Unstructured}: free form (text documents, images, audio).
\end{itemize}

\paragraph{Why formats matter.}
Structured data is easiest to analyze directly. Semi-structured data needs parsing and may have missing keys.
Unstructured data typically needs \textbf{feature extraction} (e.g., word counts from text, embeddings, image features).

\subsection*{Exercise 1 (solution)}
\textbf{Classify:}
\begin{itemize}
  \item Age: numeric (integer)
  \item Program/Branch: categorical (nominal)
  \item Attendance (\%): numeric (real)
  \item Join date: datetime
  \item Feedback comment: text
\end{itemize}

\section*{3. Data Sources and Acquisition}

\subsection*{3.1 Common sources}
\begin{itemize}
  \item \textbf{Surveys/forms}: can have missing fields and user entry errors.
  \item \textbf{Databases}: usually structured but can include stale/inconsistent codes.
  \item \textbf{Logs}: large volume, semi/unstructured, need parsing.
  \item \textbf{Sensors}: frequent readings, can have noise and missing intervals.
  \item \textbf{APIs}: provide JSON/XML, rate limits, schema changes.
\end{itemize}

\subsection*{3.2 Acquisition methods}
\begin{itemize}
  \item file import (CSV/Excel)
  \item database query (SQL)
  \item API requests (JSON)
  \item manual entry (small datasets only; double-check)
\end{itemize}

\subsection*{Exercise 2 (solution)}
\begin{itemize}
  \item Daily attendance: database export (or CSV export)
  \item Platform clicks: logs
  \item Feedback comments: survey + text field (unstructured text)
  \item Weather readings: sensors or API
\end{itemize}

\section*{4. Data Cleaning Basics}

\subsection*{4.1 What is ``dirty'' data?}
Dirty data commonly includes:
\begin{itemize}
  \item Missing values (blank, NaN, NULL)
  \item Duplicate records
  \item Inconsistent categories (\texttt{cse}, \texttt{CSE}, \texttt{ CSE})
  \item Out-of-range values (attendance 105\%, CGPA 12)
  \item Wrong type (``nine'' in a numeric column)
\end{itemize}

\subsection*{4.2 Missing values}
Missing values occur for many reasons: non-response in surveys, sensor failure, system bugs, etc.

\paragraph{Basic options.}
\begin{enumerate}
  \item \textbf{Drop rows/columns}: only if missingness is small and not biased.
  \item \textbf{Impute}: fill missing values using a rule.
  \item \textbf{Flag}: create a new column indicating missingness.
\end{enumerate}

\paragraph{Mean vs median imputation (why median is common).}
The mean is sensitive to outliers. The median is more robust.
So for a numeric column like income or CGPA, median is often a safer default imputation.

\subsection*{Exercise 3 (solution)}
If 2 values are missing out of 20:
\[
\text{missing \%} = \frac{2}{20}\times 100\% = 10\%
\]
A reasonable action: \textbf{median imputation} for CGPA and optionally add a flag column \texttt{cgpa\_was\_missing}.

\subsection*{4.3 Outliers}
An outlier is a value that looks unusually far from the rest.
Important: outliers can be \textbf{errors} or \textbf{true extremes}.
So the goal is not to automatically delete outliers; the goal is to \textbf{detect and investigate}.

\paragraph{IQR rule (fences).}
Compute:
\[
\mathrm{IQR} = Q_3 - Q_1
\]
Then:
\[
\text{Lower fence} = Q_1 - 1.5\times\mathrm{IQR},\quad
\text{Upper fence} = Q_3 + 1.5\times\mathrm{IQR}
\]
Values outside fences are flagged as possible outliers.

\subsection*{Exercise 4 (solution)}
Attendance (\%): 70, 75, 80, 85, 90, 95, 150. Median is 85.
\begin{itemize}
  \item $Q_1=75$ (median of 70,75,80)
  \item $Q_3=95$ (median of 90,95,150)
  \item IQR = $95-75=20$
  \item Fences: $75-30=45$ and $95+30=125$
  \item Since $150>125$, 150 is an outlier (by IQR rule).
\end{itemize}

\subsection*{4.4 Duplicates and inconsistent categories}
Duplicates can happen due to repeated exports, multiple submissions, or system errors.
Always check duplicates using a sensible key (e.g., \texttt{student\_id}).

Inconsistent categories occur due to case and whitespace differences.
Common fixes:
\begin{itemize}
  \item strip whitespace
  \item convert to a standard case (e.g., uppercase)
  \item map synonyms (e.g., ``Male'' and ``M'' to ``M'')
\end{itemize}

\section*{5. Mini Demo (Python)}
Run this from the lecture folder:
\begin{verbatim}
python demo/cleaning_demo.py
\end{verbatim}

The demo performs these steps:
\begin{itemize}
  \item prints shape, head, and dtypes of \texttt{data/messy\_students.csv}
  \item reports missingness and duplicates
  \item trims and standardizes categorical values (program, gender, city)
  \item converts numeric columns, parses dates
  \item flags out-of-range values and imputes numeric missing values using median
  \item removes duplicate \texttt{student\_id} rows
  \item saves \texttt{data/students\_clean.csv}
  \item saves plots in \texttt{images/} (missingness and outlier visualization)
\end{itemize}

\section*{References}
\begin{itemize}
  \item Wickham, H. \textit{Tidy Data}. Journal of Statistical Software, 2014.
  \item McKinney, W. \textit{Python for Data Analysis}. O'Reilly, 2022.
  \item Montgomery, D. C., \& Runger, G. C. \textit{Applied Statistics and Probability for Engineers}. Wiley, 7th ed., 2020.
\end{itemize}

\end{document}

